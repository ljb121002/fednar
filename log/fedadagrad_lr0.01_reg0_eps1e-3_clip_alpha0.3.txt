fedadagrad_training_loss [2.9035312873840327, 62.38959920776367, 19.193198291778565, 55.76789580627442, 7.172505192108154, 2.9716199026489267, 2.6004890530395524, 2.319019518737793, 2.321312701873779, 2.321367477722168, 2.3073527371215836, 2.304669618835449, 2.3053112530517574, 2.3100593284606945]
fedadagrad_training_accuracy [10.00000000178814, 9.99999999806285, 9.359999987334014, 10.000000016093255, 10.000000030845404, 10.345999985188246, 10.106000032573938, 10.000000052452087, 9.999999981224537, 9.999999981224537, 10.0059999717772, 10.00000000178814, 9.999999981224537, 9.999999981224537]
fedadagrad_test_loss [2.896363868713379, 62.26263845214844, 18.94864603881836, 55.70473663330078, 7.088131777954102, 2.9669877029418945, 2.5849949928283693, 2.3190157272338867, 2.3213090240478516, 2.3213635803222656, 2.3073483085632325, 2.304665476989746, 2.3053070823669435, 2.310051862335205]
fedadagrad_testing_accuracy [array(10., dtype=float32), array(10., dtype=float32), array(9.54, dtype=float32), array(10., dtype=float32), array(10., dtype=float32), array(10.28, dtype=float32), array(10.12, dtype=float32), array(10., dtype=float32), array(10., dtype=float32), array(10., dtype=float32), array(10.01, dtype=float32), array(10., dtype=float32), array(10., dtype=float32), array(10., dtype=float32)]
fedadagrad_global_learning_rate [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
